{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrangle\n",
    "import explore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.feature_selection import f_regression, SelectKBest, RFE \n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import LA Dataframe For Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle.get_zillow_data(cached=True)\n",
    "df_la, df_v, df_o = wrangle.clean_zillow_data(df)\n",
    "X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test, X_train_scaled, X_validate_scaled, X_test_scaled = wrangle.split_scale(df_la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18398, 15), (7886, 15), (6571, 15))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_validate.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>logerror</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>regionidcity</th>\n",
       "      <th>LA</th>\n",
       "      <th>Orange</th>\n",
       "      <th>Ventura</th>\n",
       "      <th>age</th>\n",
       "      <th>taxrate</th>\n",
       "      <th>acres</th>\n",
       "      <th>structure_dollar_per_sqft</th>\n",
       "      <th>land_dollar_per_sqft</th>\n",
       "      <th>bed_bath_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,855.00</td>\n",
       "      <td>32,835.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11,875,987.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1,796.19</td>\n",
       "      <td>34.12</td>\n",
       "      <td>-118.24</td>\n",
       "      <td>35,650.19</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>60.51</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.29</td>\n",
       "      <td>90.06</td>\n",
       "      <td>40.68</td>\n",
       "      <td>1.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>724,466.66</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.04</td>\n",
       "      <td>972.57</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.22</td>\n",
       "      <td>60,380.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.39</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.48</td>\n",
       "      <td>64.74</td>\n",
       "      <td>64.56</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10,711,855.00</td>\n",
       "      <td>-4.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>128.00</td>\n",
       "      <td>33.34</td>\n",
       "      <td>-118.90</td>\n",
       "      <td>3,491.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11,210,396.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1,198.00</td>\n",
       "      <td>33.96</td>\n",
       "      <td>-118.40</td>\n",
       "      <td>12,447.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.13</td>\n",
       "      <td>52.99</td>\n",
       "      <td>6.69</td>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11,892,493.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1,546.00</td>\n",
       "      <td>34.09</td>\n",
       "      <td>-118.24</td>\n",
       "      <td>14,542.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>81.36</td>\n",
       "      <td>24.92</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12,507,870.50</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2,109.00</td>\n",
       "      <td>34.20</td>\n",
       "      <td>-118.10</td>\n",
       "      <td>45,398.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>109.88</td>\n",
       "      <td>50.48</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13,102,228.00</td>\n",
       "      <td>5.26</td>\n",
       "      <td>13.00</td>\n",
       "      <td>21,929.00</td>\n",
       "      <td>34.82</td>\n",
       "      <td>-117.65</td>\n",
       "      <td>396,556.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>139.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>160.03</td>\n",
       "      <td>1,444.22</td>\n",
       "      <td>1,882.55</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  parcelid             logerror          bathroomcnt  \\\n",
       "count            32,855.00            32,855.00            32,855.00   \n",
       "mean         11,875,987.14                 0.02                 2.17   \n",
       "std             724,466.66                 0.17                 1.04   \n",
       "min          10,711,855.00                -4.66                 0.00   \n",
       "25%          11,210,396.00                -0.03                 1.00   \n",
       "50%          11,892,493.00                 0.01                 2.00   \n",
       "75%          12,507,870.50                 0.04                 3.00   \n",
       "max          13,102,228.00                 5.26                13.00   \n",
       "\n",
       "       calculatedfinishedsquarefeet             latitude            longitude  \\\n",
       "count                     32,855.00            32,855.00            32,855.00   \n",
       "mean                       1,796.19                34.12              -118.24   \n",
       "std                          972.57                 0.23                 0.22   \n",
       "min                          128.00                33.34              -118.90   \n",
       "25%                        1,198.00                33.96              -118.40   \n",
       "50%                        1,546.00                34.09              -118.24   \n",
       "75%                        2,109.00                34.20              -118.10   \n",
       "max                       21,929.00                34.82              -117.65   \n",
       "\n",
       "              regionidcity                   LA               Orange  \\\n",
       "count            32,855.00            32,855.00            32,855.00   \n",
       "mean             35,650.19                 1.00                 0.00   \n",
       "std              60,380.73                 0.00                 0.00   \n",
       "min               3,491.00                 1.00                 0.00   \n",
       "25%              12,447.00                 1.00                 0.00   \n",
       "50%              14,542.00                 1.00                 0.00   \n",
       "75%              45,398.00                 1.00                 0.00   \n",
       "max             396,556.00                 1.00                 0.00   \n",
       "\n",
       "                   Ventura                  age              taxrate  \\\n",
       "count            32,855.00            32,855.00            32,855.00   \n",
       "mean                  0.00                60.51                 0.01   \n",
       "std                   0.00                22.39                 0.01   \n",
       "min                   0.00                 1.00                 0.00   \n",
       "25%                   0.00                50.00                 0.01   \n",
       "50%                   0.00                63.00                 0.01   \n",
       "75%                   0.00                71.00                 0.01   \n",
       "max                   0.00               139.00                 0.49   \n",
       "\n",
       "                     acres  structure_dollar_per_sqft  land_dollar_per_sqft  \\\n",
       "count            32,855.00                  32,855.00             32,855.00   \n",
       "mean                  0.29                      90.06                 40.68   \n",
       "std                   2.48                      64.74                 64.56   \n",
       "min                   0.02                       0.04                  0.00   \n",
       "25%                   0.13                      52.99                  6.69   \n",
       "50%                   0.16                      81.36                 24.92   \n",
       "75%                   0.21                     109.88                 50.48   \n",
       "max                 160.03                   1,444.22              1,882.55   \n",
       "\n",
       "            bed_bath_ratio  \n",
       "count            32,835.00  \n",
       "mean                  1.66  \n",
       "std                   0.62  \n",
       "min                   0.00  \n",
       "25%                   1.33  \n",
       "50%                   1.50  \n",
       "75%                   2.00  \n",
       "max                   5.00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask = df['bed_bath_ratio'] != np.inf\n",
    "# df.loc[~mask, ' bed_bath_ratio'] = df.loc[mask, ' bed_bath_ratio'].max()\n",
    "df_la.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_scale(df):\n",
    "    train_validate, test = train_test_split(df, test_size = .2, random_state = 123)\n",
    "    train, validate = train_test_split(train_validate, test_size = .3, random_state = 123)\n",
    "    \n",
    "    # Assign variables\n",
    "    X_train = train.drop(columns=['logerror'])\n",
    "    X_validate = validate.drop(columns=['logerror'])\n",
    "    X_test = test.drop(columns=['logerror'])\n",
    "    X_train_explore = train\n",
    "\n",
    "    # I need X_train_explore set to train so I have access to the target variable.\n",
    "    y_train = train[['logerror']]\n",
    "    y_validate = validate[['logerror']]\n",
    "    y_test = test[['logerror']]\n",
    "\n",
    "    # create the scaler object and fit to X_train (get the min and max from X_train for each column)\n",
    "    scaler = MinMaxScaler(copy=True, feature_range=(0,1)).fit(X_train)\n",
    "\n",
    "    # transform X_train values to their scaled equivalent and create df of the scaled features\n",
    "    X_train_scaled = pd.DataFrame(scaler.transform(X_train), \n",
    "                                  columns=X_train.columns.values).set_index([X_train.index.values])\n",
    "    \n",
    "    # transform X_validate values to their scaled equivalent and create df of the scaled features\n",
    "    X_validate_scaled = pd.DataFrame(scaler.transform(X_validate),\n",
    "                                    columns=X_validate.columns.values).set_index([X_validate.index.values])\n",
    "\n",
    "    # transform X_test values to their scaled equivalent and create df of the scaled features   \n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), \n",
    "                                 columns=X_test.columns.values).set_index([X_test.index.values])\n",
    "    \n",
    "    return X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test, X_train_scaled, X_validate_scaled, X_test_scaled\n",
    "\n",
    "###########################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_valid_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-de928f92ccde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_explore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validate_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_valid_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_la\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_valid_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test, X_train_scaled, X_validate_scaled, X_test_scaled = train_valid_test(df_la)\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle.get_zillow_data(cached=True)\n",
    "df_v = wrangle.clean_zillow_data(df)\n",
    "X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test = wrangle.train_valid_test(df)\n",
    "X_train_scaled, X_validate_scaled, X_test_scaled = wrangle.scale_min_max(X_train, X_validate, X_test)\n",
    "X_train.shape, X_validate.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_counties(df):\n",
    "    # create dummy vars of fips id\n",
    "    county_df = pd.get_dummies(df.fips)\n",
    "    # rename columns by actual county name\n",
    "    county_df.columns = ['LA', 'Orange', 'Ventura']\n",
    "    # concatenate the dataframe with the 3 county columns to the original dataframe\n",
    "    df_dummies = pd.concat([df, county_df], axis = 1)\n",
    "    # drop regionidcounty and fips columns\n",
    "    df = df_dummies.drop(columns = ['regionidcounty', 'fips'])\n",
    "    return df\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def create_features(df):\n",
    "    df['age'] = 2017 - df.yearbuilt\n",
    "    # create taxrate variable\n",
    "    df['taxrate'] = df.taxamount/df.taxvaluedollarcnt\n",
    "    # create acres variable\n",
    "    df['acres'] = df.lotsizesquarefeet/43560\n",
    "    # dollar per square foot-structure\n",
    "    df['structure_dollar_per_sqft'] = df.structuretaxvaluedollarcnt/df.calculatedfinishedsquarefeet\n",
    "    # dollar per square foot-land\n",
    "    df['land_dollar_per_sqft'] = df.landtaxvaluedollarcnt/df.lotsizesquarefeet\n",
    "    # ratio of beds to baths\n",
    "    df['bed_bath_ratio'] = df.bedroomcnt/df.bathroomcnt\n",
    "    df['bed_bath_ratio'].round(decimals=2)\n",
    "    return df\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def remove_outliers(df):\n",
    "    '''\n",
    "    remove outliers in bed, bath, zip, square feet, acres & tax rate\n",
    "    '''\n",
    "    df[((df.bathroomcnt <= 7) & (df.bedroomcnt <= 7) & \n",
    "               (df.regionidzip < 100000) & \n",
    "               (df.bathroomcnt > 0) & \n",
    "               (df.bedroomcnt > 1) & \n",
    "               (df.acres < 10) &\n",
    "               (df.calculatedfinishedsquarefeet < 7000) & \n",
    "               (df.taxrate < .05)\n",
    "              )]\n",
    "    return df\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def col_to_drop_post_feature_creation(df):\n",
    "    cols_to_drop = ['bedroomcnt', 'taxamount', \n",
    "               'taxvaluedollarcnt', 'structuretaxvaluedollarcnt',\n",
    "               'landtaxvaluedollarcnt','lotsizesquarefeet', \"regionidzip\", \"yearbuilt\"]\n",
    "    df = df.drop(columns = cols_to_drop)\n",
    "    return df\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def county_df(df):\n",
    "    df_la = df[df.LA==1]\n",
    "    df_v = df[df.Ventura==1]\n",
    "    df_o = df[df.Orange==1]\n",
    "    return df_la, df_v, df_o\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def clean_zillow_data(df):\n",
    "    '''\n",
    "    This function drops colums that are duplicated or unneessary, creates new features, and changes column labels\n",
    "    '''\n",
    "    df.dropna(inplace=True)\n",
    "    df.latitude = df.latitude / 1000000\n",
    "    df.longitude = df.longitude / 1000000\n",
    "    df = get_counties(df)\n",
    "    df = create_features(df)\n",
    "    df = remove_outliers(df)\n",
    "    df = col_to_drop_post_feature_creation(df)\n",
    "    df_la, df_v, df_o = county_df(df)\n",
    "    return df_la, df_v, df_o\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def train_valid_test(df):\n",
    "    train_validate, test = train_test_split(df, test_size = .2, random_state = 123)\n",
    "    train, validate = train_test_split(train_validate, test_size = .3, random_state = 123)\n",
    "    \n",
    "    # Assign variables\n",
    "    X_train = train.drop(columns=['logerror'])\n",
    "    X_validate = validate.drop(columns=['logerror'])\n",
    "    X_test = test.drop(columns=['logerror'])\n",
    "    X_train_explore = train\n",
    "\n",
    "    # I need X_train_explore set to train so I have access to the target variable.\n",
    "    y_train = train[['logerror']]\n",
    "    y_validate = validate[['logerror']]\n",
    "    y_test = test[['logerror']]\n",
    "    \n",
    "    return X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def scale_min_max(X_train, X_validate, X_test):\n",
    "    # create the scaler object and fit to X_train (get the min and max from X_train for each column)\n",
    "    scaler = MinMaxScaler(copy=True, feature_range=(0,1)).fit(X_train)\n",
    "\n",
    "    # transform X_train values to their scaled equivalent and create df of the scaled features\n",
    "    X_train_scaled = pd.DataFrame(scaler.transform(X_train), \n",
    "                                  columns=X_train.columns.values).set_index([X_train.index.values])\n",
    "    \n",
    "    # transform X_validate values to their scaled equivalent and create df of the scaled features\n",
    "    X_validate_scaled = pd.DataFrame(scaler.transform(X_validate),\n",
    "                                    columns=X_validate.columns.values).set_index([X_validate.index.values])\n",
    "\n",
    "    # transform X_test values to their scaled equivalent and create df of the scaled features   \n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), \n",
    "                                 columns=X_test.columns.values).set_index([X_test.index.values])\n",
    "    \n",
    "    return X_train_scaled, X_validate_scaled, X_test_scaled\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle.get_zillow_data(cached=True)\n",
    "df_la = wrangle.clean_zillow_data(df)\n",
    "X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test = wrangle.train_valid_test(df)\n",
    "X_train_scaled, X_validate_scaled, X_test_scaled = wrangle.scale_min_max(X_train, X_validate, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test(df):\n",
    "    train_validate, test = train_test_split(df, test_size = .2, random_state = 123)\n",
    "    train, validate = train_test_split(train_validate, test_size = .3, random_state = 123)\n",
    "    \n",
    "    # Assign variables\n",
    "    X_train = train.drop(columns=['logerror'])\n",
    "    X_validate = validate.drop(columns=['logerror'])\n",
    "    X_test = test.drop(columns=['logerror'])\n",
    "    X_train_explore = train\n",
    "\n",
    "    # I need X_train_explore set to train so I have access to the target variable.\n",
    "    y_train = train[['logerror']]\n",
    "    y_validate = validate[['logerror']]\n",
    "    y_test = test[['logerror']]\n",
    "    \n",
    "    return X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test = train_valid_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call in Dataframe\n",
    "df = wrangle.get_zillow_data(cached=False)\n",
    "# Clean Data with Outliers Removed\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = wrangle.clean_zillow(df) \n",
    "# Clean Data With Outliers Scaled\n",
    "X_train_scaled, X_validate_scaled, X_test_scaled = wrangle.model_zillow(X_train, X_validate, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Split\n",
    "X_train.shape, y_train.shape, X_validate.shape, y_validate.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Scale\n",
    "X_train_scaled.shape, X_validate_scaled.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration:\n",
    "\n",
    "**Target = Logerror** \n",
    "\n",
    "\n",
    "-A number that represents a ratio that is derived from two prior distributions - the real price distribution of homes and then Zillow's existing model of that distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### New Dataframes Per County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LA County (Train)\n",
    "X_train_LA = X_train[X_train.LA==1]\n",
    "X_train_scaled_LA = X_train_scaled[X_train_scaled.LA==1]\n",
    "#################################################################\n",
    "X_validate_LA = X_validate[X_validate.LA==1]\n",
    "X_validate_scaled_LA = X_validate_scaled[X_validate_scaled.LA==1]\n",
    "#################################################################\n",
    "X_test_LA = X_test[X_test.LA==1]\n",
    "X_test_scaled_LA = X_test_scaled[X_test_scaled.LA==1]\n",
    "#################################################################\n",
    "X_train_LA.shape, X_validate_LA.shape, X_test_LA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ventura County\n",
    "X_train_V = X_train[X_train.Ventura==1]\n",
    "X_train_scaled_V = X_train_scaled[X_train_scaled.Ventura==1]\n",
    "#################################################################\n",
    "X_validate_V = X_validate[X_validate.Ventura==1]\n",
    "X_validate_scaled_V = X_validate_scaled[X_validate_scaled.Ventura==1]\n",
    "#################################################################\n",
    "X_test_V = X_test[X_test.Ventura==1]\n",
    "X_test_scaled_V= X_test_scaled[X_test_scaled.Ventura==1]\n",
    "#################################################################\n",
    "X_train_V.shape, X_validate_V.shape, X_test_V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orange County\n",
    "X_train_O = X_train[X_train.Orange==1]\n",
    "X_train_scaled_O = X_train_scaled[X_train_scaled.Orange==1]\n",
    "#################################################################\n",
    "X_validate_O = X_validate[X_validate.Orange==1]\n",
    "X_validate_scaled_O = X_validate_scaled[X_validate_scaled.Orange==1]\n",
    "#################################################################\n",
    "X_test_O = X_test[X_test.Orange==1]\n",
    "X_test_scaled_O= X_test_scaled[X_test_scaled.Orange==1]\n",
    "#################################################################\n",
    "X_train_O.shape, X_validate_O.shape, X_test_O.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LA County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_LA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inital Thoughts:\n",
    "\n",
    "- From my inital investigation on regression project I know that room count has a large affect on taxrate and housing price.  I was unable to create a derived feature last go round so I want to test the affect of this feature now.     \n",
    "\n",
    "- I want to examine how usefull our created feature of bedbathratio is in predicting logerror in LA County.  I chose LA County because it has the largest number of datapoints.  I want to cluster on bedbathratio, bathroomcnt, and caluculaedfinishedsquarefeet.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Room Clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Elbow Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reasign for formula to work correctly\n",
    "# X_train_scaled = X_train_scaled_LA.copy()\n",
    "\n",
    "cluster_vars = ['bathroomcnt', 'bed_bath_ratio', 'calculatedfinishedsquarefeet']\n",
    "explore.elbow_plot(X_train_scaled_LA, cluster_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaway:\n",
    "\n",
    "- Looks like 3 is the optimal K for this cluster\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Create Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Train Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_train_clusters, kmeans = explore.run_kmeans(X_train_LA, X_train_scaled_LA, k=3, cluster_vars=cluster_vars, cluster_col_name = 'room_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " LA_train_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize distribution of clusters, they do not look even\n",
    "LA_train_clusters.room_cluster.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_col_names = ['centroid_' + i for i in cluster_vars]\n",
    "centroid_col_names\n",
    "\n",
    "LA_centroids = pd.DataFrame(kmeans.cluster_centers_, \n",
    "             columns=centroid_col_names).reset_index().rename(columns={'index': 'room_cluster'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append cluster id onto X_train & X_train_scaled, then join with the centroids dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate cluster id on LA_X_Train\n",
    "X_train_LA_cluster = pd.concat([X_train_LA, LA_train_clusters], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LA_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join on clusterid to get centroids\n",
    "X_train_LA_cluster_centroid = X_train_LA_cluster.merge(LA_centroids, how='left', on='room_cluster').set_index(X_train_LA_cluster.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LA_cluster_centroid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusters and Centroids on Train DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize \n",
    "\n",
    "plt.scatter(X_train_LA_cluster_centroid.bathroomcnt, y_train.logerror, c=X_train_LA_cluster_centroid.room_cluster)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Validate Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_validate_clusters, kmeans = explore.run_kmeans(X_validate_LA, X_validate_scaled_LA, k=3, cluster_vars=cluster_vars, cluster_col_name = 'room_clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_validate_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c. Test Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_test_clusters, kmeans = explore.run_kmeans(X_test_LA, X_test_scaled_LA, k=3, cluster_vars=cluster_vars, cluster_col_name = 'room_clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_test_clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
