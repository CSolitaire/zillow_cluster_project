{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrangle\n",
    "import explore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.feature_selection import f_regression, SelectKBest, RFE \n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import LA Dataframe For Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28511, 16), (12219, 16), (10183, 16))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wrangle.get_zillow_data(cached=True)\n",
    "df_la, df_v, df_o = wrangle.clean_zillow_data(df)\n",
    "X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test = wrangle.train_valid_test(df)\n",
    "X_train_scaled, X_validate_scaled, X_test_scaled = wrangle.scale_min_max(X_train, X_validate, X_test)\n",
    "X_train.shape, X_validate.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32855, 16), (4350, 16), (13708, 16))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_la.shape, df_v.shape, df_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28511, 16), (12219, 16), (10183, 16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wrangle.get_zillow_data(cached=True)\n",
    "df_v = wrangle.clean_zillow_data(df)\n",
    "X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test = wrangle.train_valid_test(df)\n",
    "X_train_scaled, X_validate_scaled, X_test_scaled = wrangle.scale_min_max(X_train, X_validate, X_test)\n",
    "X_train.shape, X_validate.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_counties(df):\n",
    "    # create dummy vars of fips id\n",
    "    county_df = pd.get_dummies(df.fips)\n",
    "    # rename columns by actual county name\n",
    "    county_df.columns = ['LA', 'Orange', 'Ventura']\n",
    "    # concatenate the dataframe with the 3 county columns to the original dataframe\n",
    "    df_dummies = pd.concat([df, county_df], axis = 1)\n",
    "    # drop regionidcounty and fips columns\n",
    "    df = df_dummies.drop(columns = ['regionidcounty', 'fips'])\n",
    "    return df\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def create_features(df):\n",
    "    df['age'] = 2017 - df.yearbuilt\n",
    "    # create taxrate variable\n",
    "    df['taxrate'] = df.taxamount/df.taxvaluedollarcnt\n",
    "    # create acres variable\n",
    "    df['acres'] = df.lotsizesquarefeet/43560\n",
    "    # dollar per square foot-structure\n",
    "    df['structure_dollar_per_sqft'] = df.structuretaxvaluedollarcnt/df.calculatedfinishedsquarefeet\n",
    "    # dollar per square foot-land\n",
    "    df['land_dollar_per_sqft'] = df.landtaxvaluedollarcnt/df.lotsizesquarefeet\n",
    "    # ratio of beds to baths\n",
    "    df['bed_bath_ratio'] = df.bedroomcnt/df.bathroomcnt\n",
    "    df['bed_bath_ratio'].round(decimals=2)\n",
    "    return df\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def remove_outliers(df):\n",
    "    '''\n",
    "    remove outliers in bed, bath, zip, square feet, acres & tax rate\n",
    "    '''\n",
    "    df[((df.bathroomcnt <= 7) & (df.bedroomcnt <= 7) & \n",
    "               (df.regionidzip < 100000) & \n",
    "               (df.bathroomcnt > 0) & \n",
    "               (df.bedroomcnt > 1) & \n",
    "               (df.acres < 10) &\n",
    "               (df.calculatedfinishedsquarefeet < 7000) & \n",
    "               (df.taxrate < .05)\n",
    "              )]\n",
    "    return df\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def col_to_drop_post_feature_creation(df):\n",
    "    cols_to_drop = ['bedroomcnt', 'taxamount', \n",
    "               'taxvaluedollarcnt', 'structuretaxvaluedollarcnt',\n",
    "               'landtaxvaluedollarcnt','lotsizesquarefeet', \"regionidzip\", \"yearbuilt\"]\n",
    "    df = df.drop(columns = cols_to_drop)\n",
    "    return df\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def county_df(df):\n",
    "    df_la = df[df.LA==1]\n",
    "    df_v = df[df.Ventura==1]\n",
    "    df_o = df[df.Orange==1]\n",
    "    return df_la, df_v, df_o\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def clean_zillow_data(df):\n",
    "    '''\n",
    "    This function drops colums that are duplicated or unneessary, creates new features, and changes column labels\n",
    "    '''\n",
    "    df.dropna(inplace=True)\n",
    "    df.latitude = df.latitude / 1000000\n",
    "    df.longitude = df.longitude / 1000000\n",
    "    df = get_counties(df)\n",
    "    df = create_features(df)\n",
    "    df = remove_outliers(df)\n",
    "    df = col_to_drop_post_feature_creation(df)\n",
    "    df_la, df_v, df_o = county_df(df)\n",
    "    return df_la, df_v, df_o\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def train_valid_test(df):\n",
    "    train_validate, test = train_test_split(df, test_size = .2, random_state = 123)\n",
    "    train, validate = train_test_split(train_validate, test_size = .3, random_state = 123)\n",
    "    \n",
    "    # Assign variables\n",
    "    X_train = train.drop(columns=['logerror'])\n",
    "    X_validate = validate.drop(columns=['logerror'])\n",
    "    X_test = test.drop(columns=['logerror'])\n",
    "    X_train_explore = train\n",
    "\n",
    "    # I need X_train_explore set to train so I have access to the target variable.\n",
    "    y_train = train[['logerror']]\n",
    "    y_validate = validate[['logerror']]\n",
    "    y_test = test[['logerror']]\n",
    "    \n",
    "    return X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def scale_min_max(X_train, X_validate, X_test):\n",
    "    # create the scaler object and fit to X_train (get the min and max from X_train for each column)\n",
    "    scaler = MinMaxScaler(copy=True, feature_range=(0,1)).fit(X_train)\n",
    "\n",
    "    # transform X_train values to their scaled equivalent and create df of the scaled features\n",
    "    X_train_scaled = pd.DataFrame(scaler.transform(X_train), \n",
    "                                  columns=X_train.columns.values).set_index([X_train.index.values])\n",
    "    \n",
    "    # transform X_validate values to their scaled equivalent and create df of the scaled features\n",
    "    X_validate_scaled = pd.DataFrame(scaler.transform(X_validate),\n",
    "                                    columns=X_validate.columns.values).set_index([X_validate.index.values])\n",
    "\n",
    "    # transform X_test values to their scaled equivalent and create df of the scaled features   \n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), \n",
    "                                 columns=X_test.columns.values).set_index([X_test.index.values])\n",
    "    \n",
    "    return X_train_scaled, X_validate_scaled, X_test_scaled\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle.get_zillow_data(cached=True)\n",
    "df_la = wrangle.clean_zillow_data(df)\n",
    "X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test = wrangle.train_valid_test(df)\n",
    "X_train_scaled, X_validate_scaled, X_test_scaled = wrangle.scale_min_max(X_train, X_validate, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test(df):\n",
    "    train_validate, test = train_test_split(df, test_size = .2, random_state = 123)\n",
    "    train, validate = train_test_split(train_validate, test_size = .3, random_state = 123)\n",
    "    \n",
    "    # Assign variables\n",
    "    X_train = train.drop(columns=['logerror'])\n",
    "    X_validate = validate.drop(columns=['logerror'])\n",
    "    X_test = test.drop(columns=['logerror'])\n",
    "    X_train_explore = train\n",
    "\n",
    "    # I need X_train_explore set to train so I have access to the target variable.\n",
    "    y_train = train[['logerror']]\n",
    "    y_validate = validate[['logerror']]\n",
    "    y_test = test[['logerror']]\n",
    "    \n",
    "    return X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, X_test, X_train_explore, y_train, y_validate, y_test = train_valid_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call in Dataframe\n",
    "df = wrangle.get_zillow_data(cached=False)\n",
    "# Clean Data with Outliers Removed\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = wrangle.clean_zillow(df) \n",
    "# Clean Data With Outliers Scaled\n",
    "X_train_scaled, X_validate_scaled, X_test_scaled = wrangle.model_zillow(X_train, X_validate, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Split\n",
    "X_train.shape, y_train.shape, X_validate.shape, y_validate.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Scale\n",
    "X_train_scaled.shape, X_validate_scaled.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration:\n",
    "\n",
    "**Target = Logerror** \n",
    "\n",
    "\n",
    "-A number that represents a ratio that is derived from two prior distributions - the real price distribution of homes and then Zillow's existing model of that distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### New Dataframes Per County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LA County (Train)\n",
    "X_train_LA = X_train[X_train.LA==1]\n",
    "X_train_scaled_LA = X_train_scaled[X_train_scaled.LA==1]\n",
    "#################################################################\n",
    "X_validate_LA = X_validate[X_validate.LA==1]\n",
    "X_validate_scaled_LA = X_validate_scaled[X_validate_scaled.LA==1]\n",
    "#################################################################\n",
    "X_test_LA = X_test[X_test.LA==1]\n",
    "X_test_scaled_LA = X_test_scaled[X_test_scaled.LA==1]\n",
    "#################################################################\n",
    "X_train_LA.shape, X_validate_LA.shape, X_test_LA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ventura County\n",
    "X_train_V = X_train[X_train.Ventura==1]\n",
    "X_train_scaled_V = X_train_scaled[X_train_scaled.Ventura==1]\n",
    "#################################################################\n",
    "X_validate_V = X_validate[X_validate.Ventura==1]\n",
    "X_validate_scaled_V = X_validate_scaled[X_validate_scaled.Ventura==1]\n",
    "#################################################################\n",
    "X_test_V = X_test[X_test.Ventura==1]\n",
    "X_test_scaled_V= X_test_scaled[X_test_scaled.Ventura==1]\n",
    "#################################################################\n",
    "X_train_V.shape, X_validate_V.shape, X_test_V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orange County\n",
    "X_train_O = X_train[X_train.Orange==1]\n",
    "X_train_scaled_O = X_train_scaled[X_train_scaled.Orange==1]\n",
    "#################################################################\n",
    "X_validate_O = X_validate[X_validate.Orange==1]\n",
    "X_validate_scaled_O = X_validate_scaled[X_validate_scaled.Orange==1]\n",
    "#################################################################\n",
    "X_test_O = X_test[X_test.Orange==1]\n",
    "X_test_scaled_O= X_test_scaled[X_test_scaled.Orange==1]\n",
    "#################################################################\n",
    "X_train_O.shape, X_validate_O.shape, X_test_O.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LA County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_LA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inital Thoughts:\n",
    "\n",
    "- From my inital investigation on regression project I know that room count has a large affect on taxrate and housing price.  I was unable to create a derived feature last go round so I want to test the affect of this feature now.     \n",
    "\n",
    "- I want to examine how usefull our created feature of bedbathratio is in predicting logerror in LA County.  I chose LA County because it has the largest number of datapoints.  I want to cluster on bedbathratio, bathroomcnt, and caluculaedfinishedsquarefeet.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Room Clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Elbow Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reasign for formula to work correctly\n",
    "# X_train_scaled = X_train_scaled_LA.copy()\n",
    "\n",
    "cluster_vars = ['bathroomcnt', 'bed_bath_ratio', 'calculatedfinishedsquarefeet']\n",
    "explore.elbow_plot(X_train_scaled_LA, cluster_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaway:\n",
    "\n",
    "- Looks like 3 is the optimal K for this cluster\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Create Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Train Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_train_clusters, kmeans = explore.run_kmeans(X_train_LA, X_train_scaled_LA, k=3, cluster_vars=cluster_vars, cluster_col_name = 'room_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " LA_train_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize distribution of clusters, they do not look even\n",
    "LA_train_clusters.room_cluster.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_col_names = ['centroid_' + i for i in cluster_vars]\n",
    "centroid_col_names\n",
    "\n",
    "LA_centroids = pd.DataFrame(kmeans.cluster_centers_, \n",
    "             columns=centroid_col_names).reset_index().rename(columns={'index': 'room_cluster'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append cluster id onto X_train & X_train_scaled, then join with the centroids dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate cluster id on LA_X_Train\n",
    "X_train_LA_cluster = pd.concat([X_train_LA, LA_train_clusters], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LA_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join on clusterid to get centroids\n",
    "X_train_LA_cluster_centroid = X_train_LA_cluster.merge(LA_centroids, how='left', on='room_cluster').set_index(X_train_LA_cluster.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LA_cluster_centroid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusters and Centroids on Train DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize \n",
    "\n",
    "plt.scatter(X_train_LA_cluster_centroid.bathroomcnt, y_train.logerror, c=X_train_LA_cluster_centroid.room_cluster)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Validate Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_validate_clusters, kmeans = explore.run_kmeans(X_validate_LA, X_validate_scaled_LA, k=3, cluster_vars=cluster_vars, cluster_col_name = 'room_clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_validate_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c. Test Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_test_clusters, kmeans = explore.run_kmeans(X_test_LA, X_test_scaled_LA, k=3, cluster_vars=cluster_vars, cluster_col_name = 'room_clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_test_clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
